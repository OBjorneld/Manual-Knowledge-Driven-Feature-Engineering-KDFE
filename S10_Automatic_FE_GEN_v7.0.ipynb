{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wC9PAlnS1Sgn"},"outputs":[],"source":["###############################################\n","#Version 7.0 for S10 experiment - KDFE Pipeline\n","###############################################\n","\n","#Init installation\n","!pip install shap\n","\n","environment = 'local'\n","#colab/local\n","\n","# Basics\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","if environment == 'colab':\n","    from google.colab import files\n","    from google.colab import drive\n","\n","#Preprocessing\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from operator import itemgetter\n","#from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import RandomizedSearchCV\n","import lightgbm\n","#import shap\n","\n","#GPU\n","import tensorflow as tf\n","#tf.test.gpu_device_name()\n","\n","#Feature selection methods\n","from sklearn.feature_selection import VarianceThreshold\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2\n","from sklearn.feature_selection import mutual_info_classif\n","from sklearn.feature_selection import RFE # Recursive feature elimination\n","from sklearn.feature_selection import RFECV # with crossvalidation\n","from sklearn.svm import SVC\n","from sklearn.ensemble import ExtraTreesClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LassoCV\n","from sklearn.feature_selection import SelectFromModel\n","\n","\n","#Classification\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from xgboost import XGBClassifier\n","from sklearn.neural_network import MLPClassifier\n","from imblearn.over_sampling import SMOTE\n","\n","\n","#Evaluation\n","import time\n","from datetime import datetime, timedelta\n","from sklearn import model_selection\n","from sklearn.utils import class_weight\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics.pairwise import normalize\n","\n","\n","#Visualization\n","%matplotlib inline\n","from IPython.display import HTML, display\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","#OS\n","#Path.home()\n","#uploaded = files.upload()\n","\n","#print all rows\n","pd.set_option('display.max_rows', 20)\n","\n","#################################################################################\n","#Staging\n","#################################################################################\n","#Mount Google drive\n","drive.mount('/content/drive')\n","\n","\n","def print_border(type, length=70):\n","  border = ''\n","  for i in range(length+1):\n","    border=border+type\n","  print (border)\n","\n","def listToString(s):\n","    # initialize an empty string\n","    str1 = \"\"\n","    # traverse in the string\n","    for ele in s:\n","        str1 += ele+','\n","    # return string\n","    return str1\n","\n","#def SMOTE(y_train, ratio):\n","#  print('Performing SMOTE')\n","\n","def dfNorm(df, type):\n","    scaler = preprocessing.MinMaxScaler()\n","    columns=sorted(df)\n","    df=pd.DataFrame(scaler.fit_transform(pd.DataFrame(df)))\n","    if (type=='X'):\n","      df.columns=columns\n","    return (df)\n","\n","def var_typing (df):\n","    i = 0\n","    df['f_code']='-1'\n","    for i in range (len(df)) :\n","      feature=df['feature'].iloc[i]\n","      if (feature[0:1] =='X'):\n","        feature_code = 'ORG'\n","      else:\n","       feature_code = 'FE'\n","      df['f_code'].iloc[i]=feature_code\n","      i+=1\n","\n","def actual_time():\n","  now = datetime.now()\n","  current_time = now.strftime(\"%H:%M:%S\")\n","  return current_time\n","\n","def time_diff(starttime, endtime):\n","  if starttime == 0:\n","    tmdelta=actual_time()\n","  else:\n","    period=round(endtime-starttime)\n","    tmdelta= str(timedelta(seconds = period))\n","  return tmdelta\n","\n","def VarianceThreshold_FS(X_train,n):\n","    threshold = 0\n","    print ('Varianace threshold started with n:', n)\n","    FS_VT=VarianceThreshold(threshold=threshold)\n","    X_train_VT=FS_VT.fit_transform(X_train)\n","    print ('Size of Variance threshold before masking and type:',X_train_VT.shape)\n","    X_VT_variance=FS_VT.variances_\n","    drop_columns=X_VT_variance.argsort()[:-n]\n","    sorted_var=np.sort(FS_VT.variances_)\n","    sorted_var=sorted_var[sorted_var.argsort()[-n:]]\n","    X_VT_mask=FS_VT.get_support()\n","\n","    for i in range(len(drop_columns)):\n","      index=drop_columns[i]\n","      X_VT_mask[index] = False\n","\n","    VT_selected_features = X_train.loc[:, X_VT_mask]\n","    print ('Shape of dataset after feature selection with variance threshold:', VT_selected_features.shape)\n","    return VT_selected_features\n","\n","def KBestMutual(X_train,y_train,n):\n","    print ('KBEstMut started with n:', n)\n","    FS_KBMutual=SelectKBest(mutual_info_classif, k=n)\n","    X_KBMutual=FS_KBMutual.fit_transform(X_train, y_train)\n","    X_KBMutual_mask=FS_KBMutual.get_support()\n","    kbestN_selected_features = X_train.loc[:, X_KBMutual_mask]\n","    print ('Shape of dataset after feature selection with KBest-Mutual:', kbestN_selected_features.shape)\n","    return kbestN_selected_features\n","\n","def RFE_FS(X_train,y_train,n):\n","    print ('RFE started with n:', n)\n","    estimator =DecisionTreeClassifier()\n","    FS_RFE = RFE(estimator=estimator, n_features_to_select =n, step=1, verbose=0)\n","    X_RFE=FS_RFE.fit(X_train, y_train)\n","    X_RFE_mask=FS_RFE.get_support()\n","    RFE_selected_features = X_train.loc[:, X_RFE_mask]\n","    print ('Shape of dataset after feature selection with Recursive feature elimination (RFE):', RFE_selected_features.shape)\n","    return RFE_selected_features\n","\n","def RFECV_FS(X_train,y_train,n):\n","    print ('RFECV started with n:', n)\n","    estimator =DecisionTreeClassifier()\n","    FS_RFECV = RFECV(estimator=estimator, min_features_to_select=n, step=1, verbose=0)\n","    X_RFECV=FS_RFECV.fit(X_train, y_train)\n","    X_RFECV_mask=FS_RFECV.get_support()\n","    RFECV_selected_features = X_train.loc[:, X_RFECV_mask]\n","    print ('Shape of dataset after feature selection with Recursive feature elimination CV (RFECV):', RFECV_selected_features.shape)\n","    return RFECV_selected_features\n","\n","def VT_RFE(X_train, y_train, pre_filter_n_features, end_n_features):\n","  print('Calling VT with n:',pre_filter_n_features ,' for initial filtering')\n","  VT_selcted_features= VarianceThreshold_FS(X_train,pre_filter_n_features)\n","  print ('Starting RFE')\n","  VT_RFE_selected_features = RFE_FS(VT_selcted_features,y_train, end_n_features )\n","  return VT_RFE_selected_features\n","\n","def SModelLassoCV(X_train, y_train,n):\n","    estimator=LassoCV(cv=5)\n","    iterations=100\n","    FS_X =X_train\n","    threshold=-np.inf\n","    FS_SModelLasso=SelectFromModel(estimator, max_features=n, threshold=threshold)\n","    X_Lasso=FS_SModelLasso.fit(X_train, y_train)\n","    FS_scores = X_Lasso.estimator_.coef_\n","    if verbose:\n","      print('Ranking of features', X_Lasso.n_features_in_)\n","      print('Rank score of features', X_Lasso.estimator_.coef_)\n","      print ('Filetype for scores: ', type(X_Lasso.estimator_.coef_))\n","\n","    X_Lasso_mask=FS_SModelLasso.get_support()\n","    print ('LassoCV mask', X_Lasso_mask)\n","    SModelLassoCV_selected_features = X_train.loc[:, X_Lasso_mask]\n","    #X_SModelLasso_test_df = FS_X_test.loc[:, X_Lasso_mask]\n","    print ('Dataframe with Lasso (model selection) for n=',n,'best features')\n","    return  SModelLassoCV_selected_features\n","\n","def make_csv_df(dataset,y, sep, split,test_size, featuregroup, dataset_type):\n","    X_train_pd=[]\n","    X_test_pd=[]\n","    y_train_pd=[]\n","    y_test_pd=[]\n","\n","    df = pd.read_csv(dataset,sep=sep, low_memory = False)\n","    rows =len(df)\n","    print ('Selected dataset type and feature group:', dataset_type,'and' ,featuregroup)\n","    df=df.head(rows)\n","    if dataset_type=='EVENT':\n","      print ('chosen EVENT')\n","      if ('PK' in df.columns):\n","        df=df.drop(columns=['PK'])\n","      if (featuregroup in (['ORG','IDEATION' ,'FE-GEN'])):\n","        df=df[df['CREATION_PHASE']==featuregroup]\n","        print(featuregroup, 'chosen')\n","      if (featuregroup=='ORGIDEA'):\n","        print('ORGIDEA chosen')\n","        df=df[df['CREATION_PHASE'].isin(['ORG','IDEATION'])]\n","      if (featuregroup=='ORGFE'):\n","        print('ORGFE chosen')\n","        df=df[df['CREATION_PHASE'].isin(['ORG','FE-GEN'])]\n","      display(df.groupby(['CREATION_PHASE', 'CONCEPT_ID'])['PID'].count() )\n","      print('Unique F-group and unique patients:')\n","      display(df[['CREATION_PHASE', 'PID']].drop_duplicates().groupby(['CREATION_PHASE'])['PID'].count() )\n","      df=df.drop(columns=['CREATION_PHASE'])\n","      df=df.drop(columns=['CONCEPT_ID'])\n","    if dataset_type=='MLR':\n","      print ('chosen MLR')\n","      if ('PK' in df.columns):\n","        df=df.drop(columns=['PK'])\n","      if (featuregroup=='ORG'):\n","        df=df[org_list+y]\n","        print('ORG chosen')\n","      if (featuregroup=='IDEATION'):\n","        df=df[idea_list+y]\n","        print('IDEATION chosen')\n","      if (featuregroup=='FE-GEN'):\n","        df=df.drop(columns=org_list+idea_list)\n","        print('FE-GEN chosen')\n","      if (featuregroup=='ORGIDEA'):\n","        df= df[org_list+idea_list+y]\n","        print('ORGIDEA chosen')\n","      if (featuregroup=='ORGFE'):\n","        df=df.drop(columns=idea_list)\n","        print('ORGFE chosen')\n","\n","#Creating target dataset\n","    if verbose:\n","      print('Dropping target variable')\n","    df[y[0]].dropna()\n","    df[y[0]] = df[y[0]].astype(int)\n","    y_df=df[y[0]]\n","\n","    #Minor class ratio\n","    if verbose:\n","      print ('Type and shape of y_pd',type(y_df), y_df.shape)\n","      print ('y_df.value_counts(normalize=True)',y_df.value_counts(normalize=True))\n","    minor_class_ratio=y_df.value_counts(normalize=True).min()\n","    print('The minority rate is: ', minor_class_ratio)\n","\n","    X_df=df.drop(columns=[y[0]])\n","    #Replacing NaN with meanvalues\n","    if (imp_mean):\n","      X_df=X_df.fillna(X_df.mean())\n","      y_df=y_df.fillna(y_df.mean())\n","\n","    #Drop all NaN /zero columns and convert to integer\n","    X_df.dropna(how='all', axis=1, inplace=True)\n","\n","    #Summary of dataset\n","    X_var=X_df.columns.values\n","    print (\"Imported number of rows: \", rows,\"of\",rows, \"from dataset:\", dataset)\n","    print ('The dataset has based on featuregroup and dataset type selected:',len(X_var), 'describing features and',len(y), 'dependent variabel:')\n","    print ('The describing features are: \\n',X_var)\n","    print ('The dependent variable is:',y[0])\n","\n","    #Splitting into train and test dataset\n","    if (split):\n","      X_train, X_test, y_train_list, y_test_list = train_test_split(X_df, y_df, test_size=test_size, random_state=8675309)\n","    y_test=pd.DataFrame(y_test_list)\n","    y_train=pd.DataFrame(y_train_list)\n","    for dataset in (X_df, X_train, X_test, y_train, y_test):\n","      print ('Size of: ',dataset, ' end create dataset, ', dataset.shape)\n","\n","    return (dataset_type, X_df, y_df, X_train, X_test, y_train, y_test,minor_class_ratio)\n","\n","\n","def feature_selection (y, dataset, dataset_type, featuregroup_list,selection_methods,  normalization=0, balancing=0):\n","    '''\n","    Lightweight script to test many models and find winners\n","    :param X_train: training split\n","    :param y_train: training target vector\n","    :param X_test: test split\n","    :param y_test: test target vector\n","    '''\n","    print_border('#', 50)\n","    print ('Starting feature selection. Dataset to be used: ', dataset, ', used featuregroups: ', featuregroup_list)\n","    print_border('#', 50)\n","    results = []\n","    names = []\n","    global selected_features\n","    local_run_report_df= pd.DataFrame(columns=RUN_REPORT_COLUMNS)\n","\n","    ## Loop over featuregroups in dataset\n","    for feature_group in featuregroup_list:\n","        dataset_type, X_pd, y_pd, X_train, X_test, y_train, y_test, minor_class_ratio = make_csv_df(dataset_import, y, ';', split, test_size, feature_group, dataset_type)\n","\n","        print ('Normalization', Normalization)\n","        X_train_init=X_train\n","        X_test_init=X_test\n","        y_train_init=y_train\n","        y_test_init= y_test\n","        X_pd_init = X_pd\n","\n","        for k in Normalization:\n","          if (k=='Yes'):\n","            Norm=k\n","            print('Normalization of df performed')\n","            y_train_norm=dfNorm(y_train_init, 'y')\n","            X_train_norm=dfNorm(X_train_init, 'X')\n","          else:\n","            Norm='No'\n","            print('No normalization of df performed')\n","            y_train_norm=y_train_init\n","            X_train_norm=X_train_init\n","          #########################################\n","          #Feature selection\n","          ##########################################\n","          for FS_name, method in selection_methods:\n","              print ('Selected FS-method:', FS_name)\n","              print ('Size of dataset before FS-method',X_train_norm.shape )\n","              if FS_name == 'NO_FS':\n","                   selected_features_method = X_train_norm\n","              elif FS_name == 'VT_RFE':\n","                   n_first_filtering = 15\n","                   selected_features_method=eval(method+'( X_train_norm, y_train_norm,n_first_filtering, n)')\n","              else:\n","                  selected_features_method=eval(method+'( X_train_norm, y_train_norm,n)')\n","\n","              inner_selected_features=pd.DataFrame(sorted(selected_features_method), columns=['feature'])\n","              inner_selected_features.insert(0,'Dataset',dataset )\n","              inner_selected_features.insert(1,'Dataset type',dataset_type )\n","              inner_selected_features.insert(2,'FS_method',method )\n","              inner_selected_features.insert(3,'Norm',Norm )\n","              inner_selected_features.insert(4,'Featuregroup',feature_group )\n","              selected_features=pd.concat([selected_features, inner_selected_features], ignore_index=True)\n","              if verbose:\n","                print('Type of variables selected_features: ',type(selected_features),' and inner_selected_features:', type(inner_selected_features))\n","              print ('Norm before modeling', Norm)\n","              y_train_arr=np.array(y_train_norm).astype('int32').flatten()\n","              for i in (selected_features_method, y_train_arr):\n","                  print ('Feature selectio n ready: Shape of selected_features_method, y_train_arr:', y_train_arr.shape)\n","              y_pred= 'tomt'\n","    print_border ('-',50)\n","    print('Feature selection performed')\n","    print_border('-',50)\n","    return inner_selected_features, X_train, X_test, y_train, y_test, minor_class_ratio\n","\n","def selected_features_to_list (selected_features):\n","    print_border('#', 50)\n","    print ('Starting of creation of dataframe with selected features as lists')\n","    dataset_value_list = selected_features.Dataset.unique().tolist()\n","    FS_method_value_list= selected_features.FS_method.unique().tolist()\n","    norm_values_list=selected_features.Norm.unique().tolist()\n","    featuregroup_value_list= selected_features.Featuregroup.unique().tolist()\n","\n","    selected_features_unique=pd.DataFrame()\n","    selected_features_unique['feature_list_list']= ''\n","    selected_features_unique['selected_features_char']=''\n","    temp_df = selected_features.drop(['feature'], axis=1)\n","    selected_features_unique = temp_df.drop_duplicates(keep='first')\n","    selected_features_unique['feature_list_list']= ''\n","    print ('Unique combinations of feature groups',selected_features_unique)\n","    print('Lists with groups to iterate over' ,norm_values_list,dataset_value_list,FS_method_value_list, featuregroup_value_list )\n","    print_border('#', 100)\n","\n","    for dataset in dataset_value_list:\n","      for fs_method in FS_method_value_list:\n","        for norm in norm_values_list:\n","          for featuregroup in featuregroup_value_list:\n","              c1=selected_features['Dataset']==dataset\n","              c2=selected_features['FS_method']==fs_method\n","              c3=selected_features['Norm']==norm\n","              c4=selected_features['Featuregroup']==featuregroup\n","              conditions=c1 & c2 & c3 & c4\n","              c1_uniq=selected_features_unique['Dataset']==dataset\n","              c2_uniq=selected_features_unique['FS_method']==fs_method\n","              c3_uniq=selected_features_unique['Norm']==norm\n","              c4_uniq=selected_features_unique['Featuregroup']==featuregroup\n","              conditions_uniq = c1_uniq & c2_uniq & c3_uniq & c4_uniq\n","              if verbose:\n","                  print ('Conditions: ',conditions)\n","                  print ('Conditions for unique dataset', conditions_uniq)\n","              feature_list=selected_features.loc[c1 & c2 & c3 & c4].feature.unique().tolist()\n","              print_border('#', 50)\n","\n","              print ('Feature list: ', feature_list, ', Dataset: ', dataset, ', FS_method: ', fs_method, ', Norm: ', norm, 'Featuregroup: ',featuregroup )\n","              selected_features_unique.loc[conditions_uniq,'selected_features_char'] = listToString(feature_list)\n","              row_index=selected_features_unique.loc[conditions_uniq].index.values\n","              print ('row_index', row_index, type(row_index), len(row_index))\n","              print ('selected_features_unique: ')\n","              display(selected_features_unique)\n","              selected_features_unique.at[row_index[0], 'feature_list_list'] = np.array(feature_list)\n","    print_border('#', 100)\n","    selected_features_unique = selected_features_unique.reset_index()  # make sure indexes pair with number of rows\n","    print ('Selected features for ecah data subset, each row should be evaluated by classification evaluation ')\n","    display (selected_features_unique)\n","    return  selected_features_unique\n","\n","def model_evaluation (X_train, y_train, X_test, y_test, selected_features_list, models, minor_class_ratio, cv_outer_n,cv_inner_n, balancing=0):\n","  print_border('#', 50)\n","  Total_model_run=0\n","  performed_selected_datasets = 0\n","  global total_run_report_df\n","  total_class_report_df=pd.DataFrame(columns=RUN_REPORT_COLUMNS)\n","  for index, row in  selected_features_list.iterrows():\n","      if verbose:\n","        test_value=(minor_class_ratio < 0.33)\n","        testvalue2= test_value and balancing\n","        print ('Minority ratio', minor_class_ratio, 'balancing',balancing,'testvalues:', test_value, testvalue2)\n","      if ( (minor_class_ratio < 0.33) and balancing):\n","        print ('Performing SMOTE on training datasets (X-train, y_train) with ratio 1:2')\n","        oversample=SMOTE(sampling_strategy=0.5)\n","        X_train, y_train=oversample.fit_resample(X_train, y_train)\n","        minor_class_ratio=y_smote['Y1'].value_counts(normalize=True).min()\n","      else:\n","        print('Dataset well balanced, no need for SMOTE')\n","\n","      print('The minority rate after SMOTE is: ', minor_class_ratio)\n","      print('Sizes of datasets after SMOTE are X_train:', X_train.shape,'and y_train:', y_train.shape)\n","      print_border('#', 50)\n","      number_of_datasets=len(selected_features_list)\n","      dataset = row[1]\n","      dataset_type =row[2]\n","      FS_name=row[3]\n","      Norm=row[4]\n","      feature_group =row[5]\n","      selected_features=row[6].tolist()\n","      print ('Variabel test::', 'Dataset:', dataset, ', Datseset type:',dataset_type, ', Featuregroup:',feature_group,', Normalization:', Norm ,', FS-Method:', FS_name,', selected_features:', selected_features   )\n","      print_border('-', 20)\n","      print ('Size of X-train before: ', X_train.shape)\n","      print ('Size of X-test before: ', X_test.shape)\n","      X_train_selected=X_train[selected_features]\n","      X_test_selected=X_test[selected_features]\n","      print ('Size of X-train after selection: ', X_train_selected.shape)\n","      print ('Size of X-test after selection: ', X_test_selected.shape)\n","      print_border('-', 20)\n","      performed_selected_datasets+=1\n","\n","      for name, model in models:\n","          print('Total feature selected dataset#',number_of_datasets,', Starting:',time_diff(starttime, time.time()),', Dataset:', dataset, ', Datseset type:',dataset_type, 'Featuregroup:',feature_group,', CV outer#',cv_outer_n,', CV inner#',cv_inner_n,', Normalization:', Norm,', FS-Method:', FS_name,', Class-Model:', name,'-' ,model)\n","          cv_outer = KFold(n_splits=cv_outer_n, shuffle=True, random_state=1)\n","          model_outer_iteration_number=0\n","          print('Feature selected dataset#',performed_selected_datasets, 'of total:',number_of_datasets, 'Starting model evaluation with: ', cv_outer_n, 'outer iterations and ', cv_inner_n, ' inner iteration', ' at:', time_diff(starttime, time.time()))\n","          print_border('-', 20)\n","          # enumerate splits\n","          for train_ix_val, test_ix_val in cv_outer.split(X_train_selected):\n","            Total_model_run+=1\n","            model_outer_iteration_number+=1\n","            print ('Feature selected dataset#',performed_selected_datasets, 'of total:',number_of_datasets, 'Model outer iteration number: ', model_outer_iteration_number, ' of:', cv_outer_n,' for model: ', model, ' of', len(models) ,' at:', time_diff(starttime, time.time()) )\n","            print_border('-', 20)\n","\n","            X_train_ncv = X_train_selected.iloc[train_ix_val]\n","            X_test_ncv = X_train_selected.iloc[test_ix_val]\n","            y_train_ncv = y_train.iloc[train_ix_val]\n","            y_test_ncv =  y_train.iloc[test_ix_val]\n","            print ('Size of validation datset after splitting :: ', 'X_train_val:',X_train_ncv.shape,'X_test_val:',X_test_ncv.shape, 'y_train_val:',y_train_ncv.shape,  'y_test_val:',y_test_ncv.shape )\n","            # configure the cross-validation procedure\n","            cv_inner = KFold(n_splits=cv_inner_n, shuffle=True, random_state=1)\n","            # define search space\n","            space = dict()\n","            if (name == 'RF'):\n","              space['n_estimators'] = [10, 100, 500]\n","              space[ 'max_depth']= [10, 50, 100, None]\n","              space[ 'min_samples_leaf']= [1, 2, 4]\n","              space['max_features'] = [5]\n","            elif (name == 'LogReg'):\n","              space['penalty'] = ['l1', 'l2']\n","              space['C']= np.logspace(-5, 4, 5).tolist()\n","            elif (name=='GNB'):\n","              space['var_smoothing'] = np.logspace(0,-9, num=10)\n","            elif (name=='XGB'):\n","              space['gamma'] = [0.5, 1, 2, 5]\n","              space['subsample']= [0.75, 1.0]\n","              space['max_depth']= [3, 4, 5]\n","            elif (name=='ANN'):\n","              space['hidden_layer_sizes']=[(15,10,5),(150, 100, 50)]\n","              space['alpha']=[0.0001, 0.05]\n","            # define search\n","            print_border('#', 50)\n","            print ('Used model:',model, ', space:', space, ', model-name:', name )\n","            search = GridSearchCV(model, space, scoring='accuracy', cv=cv_inner, refit=True, error_score='raise', verbose=0)\n","            print_border('-', 20)\n","            # execute search\n","            result = search.fit(X_train_ncv, y_train_ncv.values.ravel())\n","            # get the best performing model fit on the whole training set\n","            best_model = result.best_estimator_\n","            print ('Nested crossvalidation gave the following best paramteter: ',best_model ,' for classification model',model ,' at:', time_diff(starttime, time.time()))\n","            # evaluate model on the hold out dataset\n","            yhat_ncv = best_model.predict(X_test_ncv)\n","            print ('X_test_selected - for best classs validation - prediction of yhat. Shape: ', X_test_selected.shape)\n","            yhat = best_model.predict(X_test_selected)\n","\n","            #Store the result\n","            #Adding calculations to result-df\n","            print_border('#', 50)\n","            print ('Summary of data for report summary')\n","            print ('Dataset: ',dataset,'Dataset type: ',dataset_type,'Normalizations: ',Norm, 'FS_method: ',FS_name,'Featuregroup: ',feature_group,'Balancing_needed (1/0): ',balancing,'Total_model_run#: ',Total_model_run, 'Classification_modell: ',model,'CV_outer: ',cv_outer_n,'CV-inner: ',cv_inner_n, 'Model_outer_iteration#: ',model_outer_iteration_number, 'Best_model_from_ncv',best_model  )\n","            print_border('#', 50)\n","            prediction_type_list=['test', 'ncv']\n","            metadata_list=[dataset, dataset_type,Norm, FS_name,feature_group,balancing,Total_model_run, model,cv_outer_n,cv_inner_n,model_outer_iteration_number, best_model ]\n","            local_class_metric_summary, update_row_number = classification_metric( y_test, y_test_ncv, yhat, yhat_ncv, prediction_type_list, metadata_list)\n","            print('Shape before concat:',total_class_report_df.shape,local_class_metric_summary.shape  )\n","            if Total_model_run==1:\n","              total_class_report_df=pd.DataFrame(columns=local_class_metric_summary.columns)\n","            total_class_report_df = pd.concat([total_class_report_df, local_class_metric_summary])\n","            print_border('#', 50)\n","            print ('Total class metrics summary and size: ')\n","            if reporting:\n","                print (total_class_report_df.shape)\n","                display (total_class_report_df)\n","                print_border('#', 50)\n","  return total_class_report_df\n","\n","def classification_metric (y_true, y_true_ncv, y_predicted, y_predicted_ncv, prediction_type_list, metadata_list):\n","     class_metric_summary=pd.DataFrame([metadata_list] ,columns=RUN_REPORT_COLUMNS)\n","     update_row_number=len(class_metric_summary)-1\n","     target_class_list=['class 0','class 1']\n","     metric_list=['recall', 'precision', 'f1-score']\n","     for prediction_type in prediction_type_list:\n","          if prediction_type == 'ncv':\n","                 y_true = y_true_ncv\n","                 y_predicted = y_predicted_ncv\n","          print ('Prediction metric type: ',prediction_type)\n","          report=classification_report(y_true, y_predicted, target_names=target_class_list, output_dict  = True, zero_division=1 )\n","          for target_class in target_class_list:\n","              accuracy=report['accuracy']\n","              class_metric_summary.loc[update_row_number,'accuracy_'+prediction_type]=accuracy\n","              for metric in metric_list:\n","                metric_name=metric+'_'+prediction_type\n","                class_metric_summary.loc[update_row_number, metric_name]=report[target_class][metric]\n","          metric_name= 'auroc_'+prediction_type\n","          auroc = roc_auc_score(y_true, y_predicted)\n","          class_metric_summary.loc[update_row_number, metric_name]=auroc\n","          print_border('#', 50)\n","          print ('Classification metrics for ncv iteration:')\n","          display(class_metric_summary)\n","     return class_metric_summary, update_row_number\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W2Am14fyRuTy"},"outputs":[],"source":["########################################\n","#Initialization of experiment\n","########################################\n","\n","#Used dataset\n","dataset='S10_Dataset_P2'\n","#dataset='S10_Dataset_P1_1000'\n","project=dataset[12:14]\n","dataset_import=dataset+'.csv'\n","print('Dataset_name:', dataset_import, ' and project ID:', project)\n","print_border('-')\n","\n","#Import RAW data (csv-files) from Drive\n","if environment == 'colab':\n","    #!cp \"/content/drive/MyDrive/PHD/S10/dataset/S10_Dataset_P1.csv\" S10_Dataset_P1.csv\n","    !cp \"/content/drive/MyDrive/PHD/S10/dataset/S10_Dataset_P1_1000.csv\" S10_Dataset_P1_1000.csv\n","\n","#Import locally on computer\n","dataset_import='C:/Users/olbjaa/Documents/PHD/Dataset/'+dataset_import\n","dataset='C:/Users/olbjaa/Documents/PHD/Dataset/'+dataset_import\n","\n","#Init values\n","featuregroup_list = ['ORG', 'FE-GEN', 'ORGFE', 'ORGIDEA']\n","dataset_type = 'MLR'\n","print ('Dataset type is:',dataset_type,' and featurgroups are:', featuregroup_list)\n","print_border('-')\n","\n","result_df = []\n","#selected_features = []\n","\n","imp_mean=1\n","split=True\n","test_size=0.3\n","FS_scores = []\n","if project== 'P1':\n","    org_list = [\"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\", \"X9\", \"X10\", \"X11\"]\n","elif project== 'P2':\n","    org_list = [\"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\", \"X9\", \"X10\", \"X11\", \"X12\"]\n","idea_list = []\n","y=['Y1']\n","print('The list of ORG-featrures are:', org_list, ' and IDEA-list:', idea_list)\n","print_border('-')\n","\n","\n","balancing = True\n","#Numbers of features to select\n","n=5\n","reporting = False\n","verbose=False\n","\n","cv_outer_n=10\n","cv_inner_n=3\n","\n","Normalization=['Yes', 'No']\n","\n","print ('Dataset to be used:', dataset)\n","print ('Used featuregroup:', featuregroup_list)\n","print ('Used normalization:', Normalization)\n","\n","selection_methods = [\n","('NO_FS', 'No Feature selection'),\n","('KBestMut', 'KBestMutual'),\n","('SModLassoCV', 'SModelLassoCV'),\n","('VT_RFE','VT_RFE')\n","]\n","\n","print('Total feature selection methods to be used:', len(selection_methods))\n","for j in range (len(selection_methods)):\n","  print (selection_methods[j][0])\n","\n","models = [( 'LogReg', LogisticRegression(max_iter=1000,solver='liblinear' ) ),\n","('RF', RandomForestClassifier()),\n","('GNB', GaussianNB()),\n","('XGB', XGBClassifier()),\n","('ANN', MLPClassifier())\n","]\n","\n","print_border('-')\n","print('Total classification models to be used:', len(models))\n","for j in range (len(models)):\n","  print (models[j][0])\n","\n","total_model_iterations=len(selection_methods)*len(featuregroup_list)*len(Normalization)*cv_outer_n*len(models)\n","print('Number of total model iterations', total_model_iterations)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZIgYQecLiGg","scrolled":false},"outputs":[],"source":["########################################################################\n","#Run experiment\n","########################################################################\n","\n","#from imblearn.over_sampling import SMOTE\n","##Need since some other module overrides the latest version with older\n","\n","print ('USED GPU:', tf.test.gpu_device_name())\n","\n","RUN_REPORT_COLUMNS=['Dataset','Dataset type','Normalizations','FS_method','Featuregroup','Balancing_needed','Total_model_run#' ,'Classification_modell','CV_outer','CV-inner','Model_outer_iteration#','Best_model_from_ncv' ]\n","total_run_report_df=pd.DataFrame(columns=RUN_REPORT_COLUMNS)\n","selected_features=pd.DataFrame( columns=['feature'])\n","\n","\n","starttime=time.time()\n","#starttime=time_diff(0,0)\n","print('Execution started at:',time_diff(0,starttime))\n","\n","print('\\n')\n","print_border('#', 50)\n","#1. Create dataset for each featuregroup and FS method (ml_pipeline calls create dataset and model_eval)\n","\n","inner_selected_features, X_train, X_test, y_train, y_test, minor_class_ratio = feature_selection (y, dataset,dataset_type, featuregroup_list, selection_methods, normalization=0, balancing=0)\n","selected_features_list = selected_features_to_list (selected_features)\n","\n","\n","total_class_report= model_evaluation (X_train, y_train, X_test, y_test, selected_features_list, models, minor_class_ratio, cv_outer_n,cv_inner_n, balancing)\n","\n","\n","##Export results to csv\n","sep=';'\n","file_list=[('selected_features', dataset+'_Selected_features_'+dataset_type+'_'+actual_time().replace(':', '_')+'.csv', selected_features),\n","           ('selected_features_list', dataset+'_Selected_features_list_'+dataset_type+'_'+actual_time().replace(':', '_')+'.csv', selected_features_list)\n","         ,('total_class_report', dataset+'_Classification_report_'+dataset_type+'_'+actual_time().replace(':', '_')+'.csv', total_class_report)]\n","\n","for report_file in file_list:\n","  print ('Exported object:',report_file[0] )\n","  print ('Filename of exported csv:',report_file[1] )\n","  report_file[2].to_csv(report_file[1], sep=sep)\n","  if environment == 'colab':\n","        print ('Copying file to MyDrive')\n","        files.download(report_file[1])\n","        !cp { report_file[1]} \"/content/drive/MyDrive/PHD/S10/V2/Results/P1_1000/\"\n"]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}